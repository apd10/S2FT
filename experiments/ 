[2025-04-23 20:58:07,050] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
<s> Below is an instruction that describes a task. Write a response that appropriately completes the request. 

### Instruction:
Please choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much better surgeon than Maria so _ always got the easier cases.

Option1: Sarah Option2: Maria Answer format: option1/option2

### Response:

Loading model and tokenizer...
tokenizer pad side: left
Creating model <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> from meta-llama/Llama-3.1-8B
loading updated weights from /home/ubuntu/PEFT_ARTIFACTS/models/commonsense/dora.8 | Dora=True
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128264, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): DoRALinear(shape=torch.Size([14336, 4096]), lora_dim=8, lora_dropout=Dropout(p=0.05, inplace=False), scaling=2.0)
          (down_proj): DoRALinear(shape=torch.Size([4096, 14336]), lora_dim=8, lora_dropout=Dropout(p=0.05, inplace=False), scaling=2.0)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128264, bias=False)
)
model is dtype: torch.bfloat16
-----GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}
-----
-----{'max_new_tokens': 256, 'generation_config': GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "num_beams": 4,
  "pad_token_id": 128001,
  "temperature": 0.1,
  "top_k": 40,
  "top_p": 0.75
}
}-----
Saving outputs to /home/ubuntu/NEW_ARTIFACTS/results/dora.8/commonsense//winogrande
Result 86.0, total: 1267
