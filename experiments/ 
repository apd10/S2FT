[2025-04-13 21:47:00,706] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
<s> Below is an instruction that describes a task. Write a response that appropriately completes the request. 

### Instruction:
Mr Boarden is remodeling his bathroom. For every square foot, he needs 24 mosaic tiles. How many mosaic tiles would Mr Boarden need to cover two thirds of his 36 sq ft bathroom?

### Response:

Loading model and tokenizer...
tokenizer pad side: left
Creating model <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> from meta-llama/Meta-Llama-3-8B
loading updated weights from artifacts/models/math/lora.2/ | Lora=True
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128264, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): LoRALinear(shape=torch.Size([14336, 4096]), lora_dim=2, lora_dropout=Dropout(p=0.05, inplace=False), scaling=2.0)
          (down_proj): LoRALinear(shape=torch.Size([4096, 14336]), lora_dim=2, lora_dropout=Dropout(p=0.05, inplace=False), scaling=2.0)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128264, bias=False)
)
model is dtype: torch.bfloat16
-----GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}
-----
-----{'max_new_tokens': 256, 'generation_config': GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "num_beams": 4,
  "pad_token_id": 128001,
  "temperature": 0.1,
  "top_k": 40,
  "top_p": 0.75
}
}-----
